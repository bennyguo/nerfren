<!doctype html>
<html class="no-js" lang="">

<head>
  <meta charset="utf-8">
  <title></title>
  <meta name="description" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:title" content="">
  <meta property="og:type" content="">
  <meta property="og:url" content="">
  <meta property="og:image" content="">

  <!-- Place favicon.ico in the root directory -->

  <link rel="stylesheet" href="static/css/normalize.css">
  <link rel="stylesheet" href="static/css/main.css">
  <link rel="stylesheet" href="static/css/customize.css">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-+0n0xVW2eSR5OomGNYDnhzAbDsOXxcvSN1TPprVMTNDbiYZCxYbOOl7+AMvyTG2x" crossorigin="anonymous">

  <meta name="theme-color" content="#fafafa">
</head>

<body>
  <!-- Add your site or application content here -->
  <div class="bg-light p-5 rounded-lg m-3 text-center">
    <h1 class="display-5">NeRF<span class="flip">NeR</span>:<br>Neural Radiance Fields with Reflections</h1>
    <p class="lead">Yuan-Chen Guo<sup>1</sup>&emsp;Di Kang<sup>2</sup>&emsp;Linchao Bao<sup>2</sup>&emsp;Yu He<sup>3</sup>&emsp;Song-Hai Zhang<sup>1</sup></p>
    <p class="lead">
      <sup>1</sup>BNRist, Department of Computer Science and Technology, Tsinghua University, Beijing<br>
      <sup>2</sup>Tencent AI Lab&emsp;<sup>3</sup>BIMSA
    </p>
    <hr class="my-4">
    <a class="btn btn-primary btn-lg" href="https://arxiv.org/abs/2105.06663" role="button" target="_blank">arXiv</a>
    <a class="btn btn-primary btn-lg disabled" role="button" target="_blank">Code (coming)</a>
    <a class="btn btn-primary btn-lg disabled" role="button" target="_blank">Data (Coming)</a>
  </div>
  <div class="content-wrapper">
    <div class="row justify-content-center">
      <div class="col-12">
        <div class="section-wrapper mb-5">
          <h2 class="text-center">Abstract</h2>
          <p>Neural Radiance Fields (NeRF) has achieved unprecedented view synthesis quality using coordinate-based neural scene representations. However, NeRF's view dependency can only handle simple reflections like highlights but cannot deal with complex reflections such as those from glass and mirrors. In these scenarios, NeRF models the virtual image as real geometries which leads to inaccurate depth estimation, and produces blurry renderings when the multi-view consistency is violated as the reflected objects may only be seen under some of the viewpoints. To overcome these issues, we introduce NeRFReN, which is built upon NeRF to model scenes with reflections. Specifically, we propose to split a scene into transmitted and reflected components, and model the two components with separate neural radiance fields. Considering that this decomposition is highly under-constrained, we exploit geometric priors and apply carefully-designed training strategies to achieve reasonable decomposition results. Experiments on various self-captured scenes show that our method achieves high-quality novel view synthesis and physically sound depth estimation results while enabling scene editing applications.</p>
          <div class="row justify-content-center">
            <div class="col-lg-10">
              <img src="static/media/teaser.png" class="img-fluid">
            </div>
          </div>
        </div>
        <div class="section-wrapper mb-5">
          <h2 class="text-center">Formulation</h2>
          <p>Instead of representing the whole scene with a single neural radiance field, we propose to model the <i>transmitted</i> and <i>reflected</i> parts of the scene with separate neural radiance fields. To synthesize novel views, the transmitted image \(I_t\) and reflected image \(I_r\) rendered by the corresponding fields are composed in an additive fashion, where the reflected image \(I_r\) is weighted by a learned <i>reflection fraction map</i> \(\mathbf{\beta}\): \[I = I_t + \mathbf{\beta} I_r\] The network architecture is illustrated as follow:</p>
          <div class="row justify-content-center">
            <div class="col-sm-8 col-lg-6">
              <img src="static/media/network.png" class="img-fluid">
            </div>
          </div>          
        </div>
        <div class="section-wrapper mb-5">
          <h2 class="text-center">Decomposition Results</h2>
          <p>NeRFReN is able to split the scene into transmitted and reflected components and estimate physically more correct depth (column 3) due to this decomposition. Novel views (column 1) can be synthesized by combining the transmitted (column 2) and reflected (column 5) images by the predicted reflection fraction map (column 4).</p>
          <div class="embed-responsive"><video controls autoplay loop><source src="static/media/videos/decomposition/bookcase.m4v" type="video/mp4"></div>
          <div class="embed-responsive"><video controls autoplay loop><source src="static/media/videos/decomposition/mirror.m4v" type="video/mp4"></div>
          <div class="embed-responsive"><video controls autoplay loop><source src="static/media/videos/decomposition/tv.m4v" type="video/mp4"></div>
          <div class="embed-responsive"><video controls autoplay loop><source src="static/media/videos/decomposition/art1.m4v" type="video/mp4"></div>
          <div class="embed-responsive"><video controls autoplay loop><source src="static/media/videos/decomposition/art2.m4v" type="video/mp4"></div>
          <div class="embed-responsive"><video controls autoplay loop><source src="static/media/videos/decomposition/art3.m4v" type="video/mp4"></div>
        </div>
        <div class="section-wrapper mb-5">
          <h2 class="text-center">Comparisons with NeRF</h2>
          <p>Compared with NeRF (right), NeRFReN (left) can achieve promising view synthesis results with more correct depth estimation. Benefit from the separate modeling of transmitted and reflected components, NeRFReN can handle hard cases like the <i>mirror</i> scene, where NeRF cannot synthesize correct views for some viewing directions.</p>
          <div class="row">
            <div class="col-lg-6"><div class="embed-responsive"><video controls autoplay loop><source src="static/media/videos/comparisons/bookcase.m4v" type="video/mp4"></div></div>
            <div class="col-lg-6"><div class="embed-responsive"><video controls autoplay loop><source src="static/media/videos/comparisons/mirror.m4v" type="video/mp4"></div></div>
            <div class="col-lg-6"><div class="embed-responsive"><video controls autoplay loop><source src="static/media/videos/comparisons/tv.m4v" type="video/mp4"></div></div>
            <div class="col-lg-6"><div class="embed-responsive"><video controls autoplay loop><source src="static/media/videos/comparisons/art1.m4v" type="video/mp4"></div></div>
            <div class="col-lg-6"><div class="embed-responsive"><video controls autoplay loop><source src="static/media/videos/comparisons/art2.m4v" type="video/mp4"></div></div>
            <div class="col-lg-6"><div class="embed-responsive"><video controls autoplay loop><source src="static/media/videos/comparisons/art3.m4v" type="video/mp4"></div></div>
          </div>
        </div>
        <div class="section-wrapper mb-5">
          <h2 class="text-center">Citation</h2>
          <pre class="bg-light"><code>

          </code></pre>
        </div>
      </div>
    </div>
  </div>


  <script src="static/js/vendor/modernizr-3.11.2.min.js"></script>
  <script src="static/js/plugins.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/dist/js/bootstrap.bundle.min.js" integrity="sha384-gtEjrD/SeCtmISkJkNUaaKMoLD0//ElJ19smozuHV6z3Iehds+3Ulb9Bn9Plx0x4" crossorigin="anonymous"></script>  
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>  
  <script src="static/js/main.js"></script>

</body>

</html>
